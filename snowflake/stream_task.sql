-- Test edit from local machine by Venkat Sai


USE DATABASE PRODUCT_ANALYTICS_DB;
USE SCHEMA RAW;

-- Stream on raw events (captures new rows)
CREATE OR REPLACE STREAM EVENTS_RAW_STREAM
ON TABLE EVENTS_RAW
APPEND_ONLY = TRUE;

-- Use analytics schema
USE SCHEMA ANALYTICS;

-- Warehouse for tasks (you can adjust size later)
CREATE WAREHOUSE IF NOT EXISTS WH_PRODUCT_ANALYTICS
  WAREHOUSE_SIZE = 'XSMALL'
  AUTO_SUSPEND = 60
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED = TRUE;

-- Task: Load FACT_EVENTS from STREAM
CREATE OR REPLACE TASK TASK_LOAD_FACT_EVENTS
  WAREHOUSE = WH_PRODUCT_ANALYTICS
  SCHEDULE = 'USING CRON 0 * * * * UTC'  -- every hour
AS
MERGE INTO ANALYTICS.FACT_EVENTS fe
USING (
    SELECT
        EVENT_ID,
        USER_ID,
        EVENT_NAME,
        FEATURE_NAME,
        EVENT_TS,
        CAST(EVENT_TS AS DATE) AS EVENT_DATE
    FROM RAW.EVENTS_RAW_STREAM
) s
ON fe.EVENT_ID = s.EVENT_ID
WHEN NOT MATCHED THEN
  INSERT (
    EVENT_ID,
    USER_ID,
    EVENT_NAME,
    FEATURE_NAME,
    EVENT_TS,
    EVENT_DATE
  )
  VALUES (
    s.EVENT_ID,
    s.USER_ID,
    s.EVENT_NAME,
    s.FEATURE_NAME,
    s.EVENT_TS,
    s.EVENT_DATE
  );

-- Task: Daily metrics aggregation
CREATE OR REPLACE TASK TASK_BUILD_DAILY_METRICS
  WAREHOUSE = WH_PRODUCT_ANALYTICS
  AFTER TASK_LOAD_FACT_EVENTS
AS
MERGE INTO ANALYTICS.FACT_DAILY_PRODUCT_METRICS d
USING (
    SELECT
        EVENT_DATE,
        COUNT(DISTINCT USER_ID) AS DAU,
        COUNT(*) AS EVENTS_COUNT,
        COUNT(DISTINCT FEATURE_NAME) AS UNIQUE_FEATURES_USED
    FROM ANALYTICS.FACT_EVENTS
    GROUP BY EVENT_DATE
) s
ON d.EVENT_DATE = s.EVENT_DATE
WHEN NOT MATCHED THEN
  INSERT (
    EVENT_DATE,
    DAU,
    EVENTS_COUNT,
    UNIQUE_FEATURES_USED
  )
  VALUES (
    s.EVENT_DATE,
    s.DAU,
    s.EVENTS_COUNT,
    s.UNIQUE_FEATURES_USED
  );

-- Enable tasks (you run these manually in Snowflake later)
-- ALTER TASK TASK_LOAD_FACT_EVENTS RESUME;
-- ALTER TASK TASK_BUILD_DAILY_METRICS RESUME;

